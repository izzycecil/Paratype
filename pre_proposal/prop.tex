\documentclass{article}[9pt]
\usepackage{multicol}
\usepackage[T1]{fontenc}
\usepackage[margin=0.75in]{geometry}
\usepackage{listings}
%\usepackage{microtype}

\begin{document}

%Title block
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\noindent\makebox[\textwidth][l]{\Huge Paratype --- Type Inference in
  Parallel}
\noindent\makebox[\textwidth][l]{\Large  New Mexico Tech CSE451 Project
  Proposal}

\vspace{1em}

\noindent\makebox[\textwidth][l]{\large Tyler Cecil and Ben
  Turrubiates --- \today}
\rule{\textwidth}{1pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Project}
We propose to implement a type inference system in parallel. We have
developed a small Context Free Grammar for a functional language,
using a restriction of the System F type system. In this system,
functions have return types, and a list of typed parameters. The types
themselves, however, need not be explicitly typed. Consider the
following two functions in our grammar: 
\begin{lstlisting}
  func Int foo(Int, Int){
    (x,y) = bar(x) + y
  }
  func a bar(a) {
    (x) = x
  }
\end{lstlisting}
Function foo has return type Int, and takes two parameters of type
Int. Function bar, however, has return type a, and input type a. We
refer to a as a ``type variable'' in the System F family of type
systems. However, because bar has a context ``Int bar(Int)'' (provided
by foo), at compile time we can discern the/a  (there may be multiple)
type/s of bar. For larger programs, with the ratio of explicitly typed
to implicitly typed functions approaching zero (note that there must
always exist an explicit type), this ``context dependency'' becomes
extremely large. By building this graph in parallel, we believe we can
speedup compilation or other type analysis processes. 

What we propose is an actor model of type inference. Let every
function represent a node, or actor. Actors with explicit contexts
will be able to resolve types for those which have implicit
contexts. Each actor will need to find the smallest number of contexts
to ``compile'' to. What do we mean by this? Consider an addition to our
previous example: 
\begin{lstlisting}
  func Float foo2(Float, Float){
    (x,y) = bar(x) + y
  }
  func Char foo3(Char){
    (c) = bar(c)
  }
\end{lstlisting}
Now bar has three contexts --- an Int context, a Float context, and a
Char context. Now consider we add the following type lattice. 
\begin{verbatim}
     Num
    /   \
 Int     Float
\end{verbatim}
It is the responsibility of the bar actor to fold its contexts into
Num and Char, and forward these results to other dependent function
actors. Function actors then need to constantly be adding, folding,
and sending contexts to other dependent actors. 
When all types have been resolved (if indeed possible), each actor
will export an explicitly typed copy of its definition for every
active context. The end result will be the same source file, with no
implicit typing or type variables. 

\section*{Methods}
Our actor model suggests a distributed approach to the
problem. However, compilation is a task typically performed on
comodity hardware, which is typically shared memory. Instead of using
MPI with C++, we propose using Google Go --- Google's object oriented
language with parallel primatives. Go ruitines act as threads and data
can be passed by reference through channels between ruitines. From a
code standpoint it is a message passing system, but using shared
memory. What's more, Go provides a standard BNF parser, \texttt{gopp}
which we plan to use to parse our input files.

For our analysis there are a few main factors we see: number of nodes
(or functions), average and max connectivity per node (reference
contexts), and number of process threads. We can manage number of
process threads using the \texttt{GOMAXPROCS} environment
variable. The others can be managed based on our input file. All will
need to be analyzed to build a good model of our algorithem.

\section*{Goals}

\end{document}
